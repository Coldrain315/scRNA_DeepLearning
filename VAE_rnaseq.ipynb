{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "from learn.dataset import TabularDataset\n",
    "from learn.VAE import Autoencoder,VariationalAutoencoder\n",
    "from learn.train import train_model\n",
    "\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data/\")\n",
    "# list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = pd.read_csv(data_path/\"rna_scale.csv\", index_col=0).T\n",
    "rna = rna.reset_index(drop=True)\n",
    "# rna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rna.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(rna.to_numpy(dtype=np.float32), test_size=0.1, random_state=0)\n",
    "# print(train.shape, valid.shape)\n",
    "# print(train[0])\n",
    "nfeatures = rna.shape[1]\n",
    "# print(nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TabularDataset(train)\n",
    "valid_ds = TabularDataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAE = Autoencoder(in_dims=nfeatures,latent_dims=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 50\n",
    "model, losses = train_model(modelAE, train_dl, valid_dl, lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, epochs+1, 1), y=losses['train'],\n",
    "                         mode='lines',\n",
    "                         name='train'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, epochs+1, 1), y=losses['valid'],\n",
    "                         mode='lines',\n",
    "                         name='valid'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = the number of cells\n",
    "# G = the number of genes\n",
    "# M = the number of GO terms\n",
    "# gene_indices[i] = the list of indices of genes that are in the i-th GO term\n",
    "[\n",
    "    [0, 1],\n",
    "    [2, 4, 6],\n",
    "]\n",
    "\n",
    "# create encoders\n",
    "device = 'cuda:0'\n",
    "# device = 'cpu'\n",
    "latent_dim = 20\n",
    "encoders = []\n",
    "decoders = []\n",
    "decoder_shared = Decoder(latent_dim, G).to(device)\n",
    "for gene_idx in gene_indices:\n",
    "    encoder = Encoder(len(gene_idx), latent_dim).to(device)\n",
    "    decoder = Decoder(latent_dim, len(gene_idx)).to(device)\n",
    "    encoders.append(encoder)\n",
    "    decoders.append(decoder)\n",
    "\n",
    "def step(x):\n",
    "\"\"\"\n",
    "x: a tensor of shape (batch size, G)\n",
    "\"\"\"\n",
    "    embeddings = torch.empty([M, latent_dim])\n",
    "    for i, (gene_idx, encoder) in enumerate(zip(gene_indices, encoders)):\n",
    "        embedding = encoder(x[:, gene_idx])\n",
    "        embeddings[i] = embedding\n",
    "    \n",
    "    embedding_merged = embedding.mean(0)\n",
    "    # may try self-attention\n",
    "    \n",
    "    xhat_list = []\n",
    "    loss_list = []\n",
    "    for i in range(M):\n",
    "        xhat = decoders[i](embeddings[i]) # or use embedding_merged\n",
    "        loss = criterion(xhat, x)\n",
    "        xhat_list.append(xhat)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    #\n",
    "    xhat = decoder_shared(embedding_merged)\n",
    "    loss = criterion(xhat, x)\n",
    "    \n",
    "    return loss_list\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for model in itertools.chain(encoders, decoders): model.train()\n",
    "    for x, y in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss_list = step(x)\n",
    "        for loss in loss_list: loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for model in itertools.chain(encoders, decoders): model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_total = 0\n",
    "        for x, y in test_dl:\n",
    "            loss_list = step(x)\n",
    "            for loss in loss_list: loss_total += loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
